{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfa617d-ac1e-484b-a669-bd4433a33a7b",
   "metadata": {},
   "source": [
    "# Preprocess & Features engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc1a2a-43f9-489b-a489-0bb97ee9b354",
   "metadata": {},
   "source": [
    "This notebook focuses on importing the training and testing datasets, creating dummy variables, and generating polynomial features to optimize a model for predicting housing prices. The steps are as follows:\n",
    "\n",
    "\n",
    "\n",
    "1. **Import Training and Testing Datasets** : Load the train and test data into the environment.\n",
    "\n",
    "2. **Convert Data Type for facilities Column** : Convert the data type of the facilities column from string to list for easier manipulation.\n",
    "\n",
    "3. **Function to Generate Dummy Variables** : Define a function to add dummy variables for categorical features in both the train and test datasets.\n",
    "\n",
    "4. **Function to Create Polynomial Features** : Define a function to generate polynomial features to capture non-linear relationships in both the train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e0e3-b59b-442c-a7e8-60a927882177",
   "metadata": {},
   "source": [
    "### 1. Import Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "3c70da11-6308-4b3d-a5e0-9dddf3f3374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1cd6ae29-fdfe-400a-aece-828ea22a376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data = pd.read_csv('../Dataset/bangkok_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5f37f309-6aa1-4dea-9c1a-0edac5698485",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_data =  pd.read_csv('../Dataset/test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "35d4178c-c440-4a56-95e9-b8e8aab1a662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nearby_station_distance    7014\n",
       "dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data.isnull().sum()[preprocess_data.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2cfb5-9022-4fe4-940b-a80847bbea81",
   "metadata": {},
   "source": [
    "### 2. Convert Data Type for facilities Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "88ae0911-15d6-4b0c-8102-fe66f1d07d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_facility(facilities):\n",
    "    # Check if the value is NaN (missing value)\n",
    "    if pd.isnull(facilities):\n",
    "        return 'None'  # Return 'None' if there are no facilities listed\n",
    "    \n",
    "    # If facilities is not empty, process the string\n",
    "    if facilities and len(facilities) != 0:  \n",
    "       \n",
    "        facility_text = str(facilities)[1:-1]\n",
    "        \n",
    "        facility_list = facility_text.split(',')\n",
    "        \n",
    "    return facility_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "37695809-932d-4e35-b1da-9bc453382c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_facility function to the 'facilities' column in train and test dataframe\n",
    "preprocess_data['facilities'] = preprocess_data['facilities'].apply(extract_facility)\n",
    "test_preprocess_data['facilities'] = test_preprocess_data['facilities'].apply(extract_facility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f07d30-fb15-4c64-a96f-dd0765dcb034",
   "metadata": {},
   "source": [
    "### 3. Function to Generate Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "902f5ac4-ed91-43b7-a2f2-37c8027476ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the 'nearby_station_distance' data to create separate columns for each station's distance, filling missing values with 0.\n",
    "def pivot_station(data):\n",
    "    pivot = data.pivot_table(index=data.index, \n",
    "                                      columns='station_name', \n",
    "                                      values='station_distance', \n",
    "                                      aggfunc= 'sum')\n",
    "    pivot = pivot.fillna(0)\n",
    "    pivot.columns = ['stat_' + str(col) for col in pivot.columns]\n",
    "    data = pd.concat([data.drop(columns=['nearby_station_distance','station_name','station_distance']), pivot], axis = 1)\n",
    "    return data\n",
    "    \n",
    "preprocess_data = pivot_station(preprocess_data)\n",
    "test_preprocess_data = pivot_station(test_preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7a12d102-ba46-422a-96b7-b9c0c8dd4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating dummy variables for facilities, we need to explode the list of facilities.\n",
    "def explode_facility(data):\n",
    "    # Explode the facilities list\n",
    "    facility_exploded = data.explode('facilities')\n",
    "    \n",
    "    # Create dummy variables\n",
    "    facility_dummies = pd.get_dummies(facility_exploded['facilities'].str.replace(\"'\", ''), prefix='faci')\n",
    "    \n",
    "    # Group by the original index and sum the dummy variables\n",
    "    facility_pivoted = facility_dummies.groupby(facility_exploded.index).sum()\n",
    "    \n",
    "    # Concatenate the dummy variables with the original DataFrame after dropping the 'facilities' column\n",
    "    data = pd.concat([data.drop(columns=['facilities']), facility_pivoted], axis=1)\n",
    "\n",
    "    return data\n",
    "    \n",
    "preprocess_data = explode_facility(preprocess_data)\n",
    "test_preprocess_data = explode_facility(test_preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "262777e6-f2b1-4532-8cb5-084c3bfe3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For these categorical columns, we can create dummy variables directly.\n",
    "preprocess_data = pd.get_dummies(data=preprocess_data, columns=[\"property_type\", \"district\", \"province\"])\n",
    "test_preprocess_data = pd.get_dummies(data=test_preprocess_data, columns=[\"property_type\", \"district\", \"province\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835dc4d-f6d5-44eb-81e7-3adbb9102315",
   "metadata": {},
   "source": [
    "### 4. Function to Create Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "e37e8ffc-7b38-4cbc-9044-48f184bd1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feature(data):\n",
    "    # Select the initial features for polynomial expansion\n",
    "    starter_features = ['bedrooms','baths','floor_area', 'total_facilities']\n",
    "    \n",
    "    # Select features for polynomial transformation\n",
    "    data_poly = data[starter_features]\n",
    "    \n",
    "    # Generate polynomial features of degree 3 without the bias term\n",
    "    poly = PolynomialFeatures(include_bias=False, degree=3)\n",
    "    X_poly = poly.fit_transform(data_poly)\n",
    "    \n",
    "    # Create a DataFrame for polynomial features and concatenate with the original data\n",
    "    poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(starter_features))\n",
    "    data = pd.concat([data, poly_df], axis=1)\n",
    "    data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "    return data\n",
    "\n",
    "preprocess_data = poly_feature(preprocess_data)\n",
    "test_preprocess_data = poly_feature(test_preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3ca86-3c6c-44a2-aa0a-10529c27ee7d",
   "metadata": {},
   "source": [
    "### Save file to tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b10af387-0649-467e-9e0a-872020957039",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data.to_csv('../Dataset/bangkok_preprocess.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "25b65135-eb49-4fb9-be67-17766b4f5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_data.to_csv('../Dataset/test_preprocess.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
